{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe32d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076e909",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5cd444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n",
      "search_path = stg, mart, public\n",
      "who/where   = ('jet2_holiday', 'dbmasteruser')\n",
      "\n",
      "Tables in schema stg: ['accident', 'claim', 'driver', 'policyholder', 'vehicle']\n",
      "Loading table stg.accident ...\n",
      "Loading table stg.claim ...\n",
      "Loading table stg.driver ...\n",
      "Loading table stg.policyholder ...\n",
      "Loading table stg.vehicle ...\n",
      "\n",
      "Done. Loaded tables: ['accident', 'claim', 'driver', 'policyholder', 'vehicle']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_number</th>\n",
       "      <th>subrogation</th>\n",
       "      <th>claim_est_payout</th>\n",
       "      <th>liab_prct</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>claim_day_of_week</th>\n",
       "      <th>channel</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>witness_present_ind</th>\n",
       "      <th>policy_report_filed_ind</th>\n",
       "      <th>in_network_bodyshop</th>\n",
       "      <th>accident_key</th>\n",
       "      <th>policyholder_key</th>\n",
       "      <th>vehicle_key</th>\n",
       "      <th>driver_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6090851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3218.84</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12/4/2016</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Broker</td>\n",
       "      <td>80040.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4653734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1338.52</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4/25/2015</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Phone</td>\n",
       "      <td>80030.0</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1014777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3540.05</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6/22/2015</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Broker</td>\n",
       "      <td>50012.0</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8101873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1507.94</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3/2/2015</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Phone</td>\n",
       "      <td>20138.0</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5081870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5080.63</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1/12/2016</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Online</td>\n",
       "      <td>50033.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_number  subrogation  claim_est_payout  liab_prct claim_date  \\\n",
       "0       6090851          1.0           3218.84       31.0  12/4/2016   \n",
       "1       4653734          0.0           1338.52       34.0  4/25/2015   \n",
       "2       1014777          0.0           3540.05       39.0  6/22/2015   \n",
       "3       8101873          1.0           1507.94       32.0   3/2/2015   \n",
       "4       5081870          0.0           5080.63       28.0  1/12/2016   \n",
       "\n",
       "  claim_day_of_week channel  zip_code witness_present_ind  \\\n",
       "0          Saturday  Broker   80040.0                   Y   \n",
       "1         Wednesday   Phone   80030.0                   N   \n",
       "2          Thursday  Broker   50012.0                   N   \n",
       "3          Saturday   Phone   20138.0                   N   \n",
       "4            Sunday  Online   50033.0                   N   \n",
       "\n",
       "   policy_report_filed_ind in_network_bodyshop  accident_key  \\\n",
       "0                      1.0                  no           1.0   \n",
       "1                      1.0                 yes           1.0   \n",
       "2                      1.0                 yes           2.0   \n",
       "3                      1.0                 yes           2.0   \n",
       "4                      0.0                 yes           3.0   \n",
       "\n",
       "   policyholder_key  vehicle_key  driver_key  \n",
       "0               1.0          1.0         1.0  \n",
       "1               2.0          2.0         2.0  \n",
       "2               3.0          3.0         3.0  \n",
       "3               4.0          4.0         4.0  \n",
       "4               5.0          5.0         5.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 1. Read DB connection info from environment variables\n",
    "PGHOST = os.getenv(\"PGHOST\")\n",
    "PGPORT = os.getenv(\"PGPORT\", \"5432\")\n",
    "PGDB = os.getenv(\"PGDATABASE\")\n",
    "PGUSER = os.getenv(\"PGUSER\")\n",
    "PGPASS = os.getenv(\"PGPASSWORD\")\n",
    "PGSSL = os.getenv(\"PGSSLMODE\", \"require\")\n",
    "\n",
    "# 2. Create SQLAlchemy engine for Postgres\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{PGUSER}:{PGPASS}@{PGHOST}:{PGPORT}/{PGDB}\"\n",
    "    + (f\"?sslmode={PGSSL}\" if PGSSL else \"\")\n",
    ")\n",
    "\n",
    "# 3. Test connection and fetch table names from schema `stg`\n",
    "with engine.begin() as con:\n",
    "    print(\"Connection successful.\")\n",
    "    print(\"search_path =\", con.execute(text(\"SHOW search_path\")).scalar())\n",
    "    print(\n",
    "        \"who/where   =\",\n",
    "        con.execute(text(\"SELECT current_database(), current_user\")).fetchone(),\n",
    "    )\n",
    "\n",
    "    # Get all table names under schema `stg`\n",
    "    table_names = con.execute(\n",
    "        text(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'stg'\n",
    "        ORDER BY table_name\n",
    "        \"\"\")\n",
    "    ).scalars().all()\n",
    "\n",
    "print(\"\\nTables in schema stg:\", table_names)\n",
    "\n",
    "# 4. Load each table into a pandas DataFrame\n",
    "dfs = {}  # dict: {table_name: DataFrame}\n",
    "\n",
    "for t in table_names:\n",
    "    print(f\"Loading table stg.{t} ...\")\n",
    "    # Use double quotes around table name in case of capitals/special chars\n",
    "    query = f'SELECT * FROM stg.\"{t}\"'\n",
    "    dfs[t] = pd.read_sql(query, engine)\n",
    "\n",
    "print(\"\\nDone. Loaded tables:\", list(dfs.keys()))\n",
    "\n",
    "# 5. Examples: access individual tables for joins / feature engineering\n",
    "accident_df = dfs[\"accident\"]\n",
    "claim_df = dfs[\"claim\"]\n",
    "driver_df = dfs[\"driver\"]\n",
    "policyholder_df = dfs[\"policyholder\"]\n",
    "vehicle_df = dfs[\"vehicle\"]\n",
    "\n",
    "# Example: quick preview\n",
    "claim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec83c9",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8a646b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18001 entries, 0 to 18000\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   claim_number             18001 non-null  int64  \n",
      " 1   subrogation              17999 non-null  float64\n",
      " 2   claim_est_payout         18000 non-null  float64\n",
      " 3   liab_prct                18000 non-null  float64\n",
      " 4   claim_date               18000 non-null  object \n",
      " 5   claim_day_of_week        18000 non-null  object \n",
      " 6   channel                  18000 non-null  object \n",
      " 7   zip_code                 18000 non-null  float64\n",
      " 8   witness_present_ind      18000 non-null  object \n",
      " 9   policy_report_filed_ind  18000 non-null  float64\n",
      " 10  in_network_bodyshop      18000 non-null  object \n",
      " 11  accident_key             18001 non-null  int64  \n",
      " 12  policyholder_key         18001 non-null  int64  \n",
      " 13  vehicle_key              18001 non-null  int64  \n",
      " 14  driver_key               18001 non-null  int64  \n",
      " 15  accident_site            18000 non-null  object \n",
      " 16  accident_type            18000 non-null  object \n",
      " 17  annual_income            18000 non-null  float64\n",
      " 18  high_education_ind       18000 non-null  float64\n",
      " 19  email_or_tel_available   18000 non-null  float64\n",
      " 20  address_change_ind       18000 non-null  float64\n",
      " 21  living_status            18000 non-null  object \n",
      " 22  past_num_of_claims       18000 non-null  float64\n",
      " 23  vehicle_made_year        18000 non-null  float64\n",
      " 24  vehicle_category         18000 non-null  object \n",
      " 25  vehicle_price            18000 non-null  float64\n",
      " 26  vehicle_color            18000 non-null  object \n",
      " 27  vehicle_weight           18000 non-null  float64\n",
      " 28  vehicle_mileage          18000 non-null  float64\n",
      " 29  year_of_born             18000 non-null  float64\n",
      " 30  gender                   18000 non-null  object \n",
      " 31  age_of_DL                18000 non-null  float64\n",
      " 32  safety_rating            18000 non-null  float64\n",
      "dtypes: float64(17), int64(5), object(11)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "for col in [\"accident_key\", \"policyholder_key\", \"vehicle_key\", \"driver_key\"]:\n",
    "    claim_df[col] = claim_df[col].fillna(0).astype(int)\n",
    "    claim_df[col] = claim_df[col].astype(\"int64\")\n",
    "\n",
    "df = (claim_df\n",
    "      .merge(accident_df, on=\"accident_key\", how=\"left\")\n",
    "      .merge(policyholder_df, on=\"policyholder_key\", how=\"left\")\n",
    "      .merge(vehicle_df, on=\"vehicle_key\", how=\"left\")\n",
    "      .merge(driver_df, on=\"driver_key\", how=\"left\"))\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b4322",
   "metadata": {},
   "source": [
    "## Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a0a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_features_v2(df, artifacts=None):\n",
    "    is_training = (artifacts is None)\n",
    "    if is_training:\n",
    "        artifacts = {}\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # TIME\n",
    "    df['claim_date'] = pd.to_datetime(df['claim_date'], errors='coerce')\n",
    "    df['claim_year'] = df['claim_date'].dt.year\n",
    "    df['claim_month'] = df['claim_date'].dt.month\n",
    "    df['claim_dow'] = df['claim_date'].dt.dayofweek\n",
    "    df['claim_hour'] = df['claim_date'].dt.hour\n",
    "    df['claim_day'] = df['claim_date'].dt.day\n",
    "    df['is_weekend'] = (df['claim_dow'] >= 5).astype(int)\n",
    "    df['is_weekday'] = (df['claim_dow'] < 5).astype(int)\n",
    "    df['is_morning'] = ((df['claim_hour'] >= 6) & (\n",
    "        df['claim_hour'] < 12)).astype(int)\n",
    "    df['is_afternoon'] = ((df['claim_hour'] >= 12) &\n",
    "                          (df['claim_hour'] < 18)).astype(int)\n",
    "    df['is_evening'] = ((df['claim_hour'] >= 18) & (\n",
    "        df['claim_hour'] < 22)).astype(int)\n",
    "    df['is_night'] = ((df['claim_hour'] >= 22) | (\n",
    "        df['claim_hour'] < 6)).astype(int)\n",
    "    df['is_rush_hour'] = (((df['claim_hour'] >= 7) & (df['claim_hour'] <= 9)) |\n",
    "                          ((df['claim_hour'] >= 16) & (df['claim_hour'] <= 19))).astype(int)\n",
    "    df['claim_quarter'] = ((df['claim_month'] - 1) // 3 + 1)\n",
    "    df['is_winter'] = df['claim_month'].isin([12, 1, 2]).astype(int)\n",
    "    df['is_summer'] = df['claim_month'].isin([6, 7, 8]).astype(int)\n",
    "\n",
    "    # DEMOGRAPHICS\n",
    "    df['year_of_born'] = pd.to_numeric(\n",
    "        df['year_of_born'], errors='coerce').fillna(1980)\n",
    "    df['age_of_DL'] = pd.to_numeric(\n",
    "        df['age_of_DL'], errors='coerce').fillna(25)\n",
    "    df['age_at_claim'] = (df['claim_year'] - df['year_of_born']).clip(16, 100)\n",
    "    df['period_of_driving'] = (\n",
    "        df['age_at_claim'] - df['age_of_DL']).clip(lower=0)\n",
    "    df['is_young_driver'] = (df['age_at_claim'] < 25).astype(int)\n",
    "    df['is_senior_driver'] = (df['age_at_claim'] >= 65).astype(int)\n",
    "    df['is_mid_age_driver'] = ((df['age_at_claim'] >= 25) & (\n",
    "        df['age_at_claim'] < 65)).astype(int)\n",
    "    df['is_new_driver'] = (df['period_of_driving'] < 3).astype(int)\n",
    "    df['is_experienced'] = (df['period_of_driving'] >= 10).astype(int)\n",
    "\n",
    "    # CLAIMS\n",
    "    df['past_num_of_claims'] = pd.to_numeric(\n",
    "        df['past_num_of_claims'], errors='coerce').fillna(0)\n",
    "    df['claims_per_year'] = df['past_num_of_claims'] / \\\n",
    "        (df['period_of_driving'] + 1)\n",
    "    df['has_past_claims'] = (df['past_num_of_claims'] > 0).astype(int)\n",
    "    df['has_multiple_claims'] = (df['past_num_of_claims'] >= 2).astype(int)\n",
    "\n",
    "    # MILEAGE\n",
    "    vm = pd.to_numeric(df['vehicle_mileage'], errors='coerce')\n",
    "    if is_training:\n",
    "        artifacts['mileage_median'] = vm.median()\n",
    "    df['vehicle_mileage'] = vm.fillna(artifacts['mileage_median'])\n",
    "    df['mileage_per_year'] = df['vehicle_mileage'] / \\\n",
    "        (df['period_of_driving'] + 1)\n",
    "    df['mileage_log'] = np.log1p(df['vehicle_mileage'])\n",
    "    df['is_high_mileage'] = (df['vehicle_mileage'] >\n",
    "                             df['vehicle_mileage'].quantile(0.75)).astype(int)\n",
    "\n",
    "    # FINANCIAL\n",
    "    for col in ['annual_income', 'vehicle_price', 'vehicle_weight', 'claim_est_payout']:\n",
    "        val = pd.to_numeric(df[col], errors='coerce')\n",
    "        if is_training:\n",
    "            artifacts[f'{col}_med'] = val.median()\n",
    "            artifacts[f'{col}_p99'] = val.quantile(0.99)\n",
    "            artifacts[f'{col}_p01'] = val.quantile(0.01)\n",
    "        val = val.fillna(artifacts[f'{col}_med'])\n",
    "        val = val.clip(artifacts[f'{col}_p01'], artifacts[f'{col}_p99'])\n",
    "        df[f'{col}_capped'] = val\n",
    "        df[f'{col}_log'] = np.log1p(val)\n",
    "\n",
    "    df['payout_to_income'] = df['claim_est_payout_capped'] / \\\n",
    "        (df['annual_income_capped'] + 1)\n",
    "    df['payout_to_price'] = df['claim_est_payout_capped'] / \\\n",
    "        (df['vehicle_price_capped'] + 1)\n",
    "    df['income_to_price'] = df['annual_income_capped'] / \\\n",
    "        (df['vehicle_price_capped'] + 1)\n",
    "    df['is_high_income'] = (df['annual_income_capped'] >\n",
    "                            df['annual_income_capped'].quantile(0.75)).astype(int)\n",
    "    df['is_expensive_car'] = (df['vehicle_price_capped'] >\n",
    "                              df['vehicle_price_capped'].quantile(0.75)).astype(int)\n",
    "    df['is_large_payout'] = (df['claim_est_payout_capped'] >\n",
    "                             df['claim_est_payout_capped'].quantile(0.75)).astype(int)\n",
    "\n",
    "    # LIABILITY\n",
    "    df['liab_prct'] = pd.to_numeric(\n",
    "        df['liab_prct'], errors='coerce').fillna(0).clip(0, 100)\n",
    "    df['liab_0_10'] = (df['liab_prct'] <= 10).astype(int)\n",
    "    df['liab_10_20'] = ((df['liab_prct'] > 10) & (\n",
    "        df['liab_prct'] <= 20)).astype(int)\n",
    "    df['liab_20_30'] = ((df['liab_prct'] > 20) & (\n",
    "        df['liab_prct'] <= 30)).astype(int)\n",
    "    df['liab_30_40'] = ((df['liab_prct'] > 30) & (\n",
    "        df['liab_prct'] <= 40)).astype(int)\n",
    "    df['liab_40_plus'] = (df['liab_prct'] > 40).astype(int)\n",
    "\n",
    "    for i in range(0, 100, 5):\n",
    "        df[f'liab_{i}_{i+5}'] = ((df['liab_prct'] > i)\n",
    "                                 & (df['liab_prct'] <= i+5)).astype(int)\n",
    "\n",
    "    for val in [15, 18, 20, 22, 25, 27, 30, 32, 35, 37, 40, 45, 50]:\n",
    "        df[f'liab_exactly_{val}'] = (df['liab_prct'] == val).astype(int)\n",
    "\n",
    "    df['liab_squared'] = df['liab_prct'] ** 2\n",
    "    df['liab_cubed'] = df['liab_prct'] ** 3\n",
    "    df['liab_sqrt'] = np.sqrt(df['liab_prct'])\n",
    "    df['liab_inverse'] = 100 - df['liab_prct']\n",
    "    df['liab_inverse_sq'] = df['liab_inverse'] ** 2\n",
    "    df['liab_log'] = np.log1p(df['liab_prct'])\n",
    "    df['liab_zero'] = (df['liab_prct'] == 0).astype(int)\n",
    "    df['liab_full'] = (df['liab_prct'] == 100).astype(int)\n",
    "    df['liab_half'] = (df['liab_prct'] == 50).astype(int)\n",
    "\n",
    "    # EVIDENCE\n",
    "    df['has_witness'] = df['witness_present_ind'].fillna(\n",
    "        'N').str.upper().isin(['Y', 'YES', '1', 'TRUE']).astype(int)\n",
    "    df['has_police'] = pd.to_numeric(\n",
    "        df['policy_report_filed_ind'], errors='coerce').fillna(0).astype(int)\n",
    "    df['evidence_count'] = df['has_witness'] + df['has_police']\n",
    "    df['has_full_evidence'] = (df['evidence_count'] == 2).astype(int)\n",
    "    df['has_no_evidence'] = (df['evidence_count'] == 0).astype(int)\n",
    "    df['in_network'] = df['in_network_bodyshop'].fillna(\n",
    "        'no').str.lower().isin(['yes', 'y', '1']).astype(int)\n",
    "\n",
    "    # PROFILE\n",
    "    df['high_education'] = pd.to_numeric(\n",
    "        df['high_education_ind'], errors='coerce').fillna(0).astype(int)\n",
    "    df['address_change'] = pd.to_numeric(\n",
    "        df['address_change_ind'], errors='coerce').fillna(0).astype(int)\n",
    "    df['safety_rating'] = pd.to_numeric(\n",
    "        df['safety_rating'], errors='coerce').fillna(50)\n",
    "    df['safety_high'] = (df['safety_rating'] >= 70).astype(int)\n",
    "    df['safety_low'] = (df['safety_rating'] <= 30).astype(int)\n",
    "\n",
    "    # ACCIDENT\n",
    "    df['accident_type'] = df['accident_type'].fillna('Unknown').astype(str)\n",
    "    df['accident_site'] = df['accident_site'].fillna('Unknown').astype(str)\n",
    "    df['is_single_car'] = df['accident_type'].str.contains(\n",
    "        'single', case=False, na=False).astype(int)\n",
    "    df['is_multi_unclear'] = df['accident_type'].str.contains(\n",
    "        'multi.*unclear', case=False, na=False).astype(int)\n",
    "    df['is_multi_clear'] = df['accident_type'].str.contains(\n",
    "        'multi.*clear', case=False, na=False).astype(int)\n",
    "    df['is_highway'] = df['accident_site'].str.contains(\n",
    "        'highway', case=False, na=False).astype(int)\n",
    "    df['is_intersection'] = df['accident_site'].str.contains(\n",
    "        'intersection', case=False, na=False).astype(int)\n",
    "    df['is_parking'] = df['accident_site'].str.contains(\n",
    "        'parking', case=False, na=False).astype(int)\n",
    "\n",
    "    # INTERACTIONS\n",
    "    df['liab_x_witness'] = df['liab_prct'] * df['has_witness']\n",
    "    df['liab_x_police'] = df['liab_prct'] * df['has_police']\n",
    "    df['liab_x_evidence_count'] = df['liab_prct'] * df['evidence_count']\n",
    "    df['liab_inverse_x_evidence'] = df['liab_inverse'] * df['evidence_count']\n",
    "    df['liab_20_30_x_multi_unclear'] = df['liab_20_30'] * df['is_multi_unclear']\n",
    "    df['liab_20_30_x_single'] = df['liab_20_30'] * df['is_single_car']\n",
    "    df['low_liab_x_multi'] = (df['liab_prct'] < 30).astype(\n",
    "        int) * (1 - df['is_single_car'])\n",
    "    df['high_liab_x_single'] = (df['liab_prct'] > 50).astype(\n",
    "        int) * df['is_single_car']\n",
    "    df['liab_x_highway'] = df['liab_prct'] * df['is_highway']\n",
    "    df['liab_x_intersection'] = df['liab_prct'] * df['is_intersection']\n",
    "    df['liab_x_weekend'] = df['liab_prct'] * df['is_weekend']\n",
    "    df['liab_x_rush_hour'] = df['liab_prct'] * df['is_rush_hour']\n",
    "    df['liab_x_night'] = df['liab_prct'] * df['is_night']\n",
    "    df['liab_x_young_driver'] = df['liab_prct'] * df['is_young_driver']\n",
    "    df['liab_x_new_driver'] = df['liab_prct'] * df['is_new_driver']\n",
    "    df['liab_inverse_x_experienced'] = df['liab_inverse'] * df['is_experienced']\n",
    "    df['liab_x_past_claims'] = df['liab_prct'] * df['has_past_claims']\n",
    "    df['liab_inverse_x_no_claims'] = df['liab_inverse'] * \\\n",
    "        (1 - df['has_past_claims'])\n",
    "    df['liab_x_payout_ratio'] = df['liab_prct'] * df['payout_to_income']\n",
    "    df['liab_inverse_x_high_income'] = df['liab_inverse'] * df['is_high_income']\n",
    "    df['liab_20_30_x_multi_x_evidence'] = df['liab_20_30'] * \\\n",
    "        df['is_multi_unclear'] * (df['evidence_count'] > 0).astype(int)\n",
    "    df['low_liab_x_single_x_no_evidence'] = (df['liab_prct'] < 25).astype(\n",
    "        int) * df['is_single_car'] * df['has_no_evidence']\n",
    "    df['high_liab_x_weekend_x_night'] = (df['liab_prct'] > 60).astype(\n",
    "        int) * df['is_weekend'] * df['is_night']\n",
    "    df['golden_combo'] = df['liab_20_30'] * df['is_multi_unclear'] * \\\n",
    "        (df['evidence_count'] > 0).astype(int) * df['is_highway']\n",
    "\n",
    "    # CATEGORICALS\n",
    "    cat_cols = ['gender', 'vehicle_category', 'channel']\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna('Unknown').astype(str)\n",
    "\n",
    "    df['accident_combo'] = df['accident_site'] + '_' + df['accident_type']\n",
    "    df['zip_code'] = pd.to_numeric(df['zip_code'], errors='coerce').fillna(\n",
    "        0).astype(int).astype(str).str.zfill(5)\n",
    "    df['zip3'] = df['zip_code'].str[:3]\n",
    "    df['zip3'] = df['zip3'].where(df['zip3'] != '000', 'unknown')\n",
    "\n",
    "    if is_training:\n",
    "        return df, artifacts\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95bd4e",
   "metadata": {},
   "source": [
    "## Target Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9844fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(X_train, y_train, X_val, X_test, cols, smoothing=30):\n",
    "    global_mean = y_train.mean()\n",
    "    te_names = []\n",
    "\n",
    "    for col in cols:\n",
    "        agg = pd.DataFrame({col: X_train[col], 'y': y_train}).groupby(col)[\n",
    "            'y'].agg(['sum', 'count'])\n",
    "        agg['mean'] = (agg['sum'] + smoothing * global_mean) / \\\n",
    "            (agg['count'] + smoothing)\n",
    "        m = agg['mean'].to_dict()\n",
    "        te_col = f'{col}_te'\n",
    "        X_train[te_col] = X_train[col].map(m).fillna(global_mean)\n",
    "        X_val[te_col] = X_val[col].map(m).fillna(global_mean)\n",
    "        X_test[te_col] = X_test[col].map(m).fillna(global_mean)\n",
    "        te_names.append(te_col)\n",
    "\n",
    "    return te_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6a0c6",
   "metadata": {},
   "source": [
    "## Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0096c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP-selected features from a previous run\n",
    "SELECTED_FEATURES = [\n",
    "    'is_single_car', 'liab_x_witness', 'liab_inverse', 'liab_x_highway', 'has_witness',\n",
    "    'high_education', 'address_change', 'is_parking', 'liab_40_plus', 'is_multi_clear',\n",
    "    'liab_prct', 'liab_20_30', 'liab_squared', 'liab_cubed', 'liab_sqrt', 'liab_inverse_sq',\n",
    "    'liab_log', 'liab_zero', 'liab_full', 'liab_half', 'liab_0_10', 'liab_10_20', 'liab_30_40',\n",
    "    'liab_x_police', 'liab_x_evidence_count', 'liab_inverse_x_evidence', 'liab_20_30_x_multi_unclear',\n",
    "    'liab_20_30_x_single', 'low_liab_x_multi', 'high_liab_x_single', 'liab_x_intersection',\n",
    "    'liab_x_weekend', 'liab_x_rush_hour', 'liab_x_night', 'liab_x_young_driver', 'liab_x_new_driver',\n",
    "    'liab_inverse_x_experienced', 'liab_x_past_claims', 'liab_inverse_x_no_claims', 'liab_x_payout_ratio',\n",
    "    'liab_inverse_x_high_income', 'liab_20_30_x_multi_x_evidence', 'low_liab_x_single_x_no_evidence',\n",
    "    'high_liab_x_weekend_x_night', 'golden_combo', 'has_police', 'evidence_count', 'has_full_evidence',\n",
    "    'has_no_evidence', 'in_network', 'safety_rating', 'safety_high', 'safety_low', 'is_multi_unclear',\n",
    "    'is_highway', 'is_intersection', 'age_at_claim', 'period_of_driving', 'is_young_driver',\n",
    "    'is_senior_driver', 'is_mid_age_driver', 'is_new_driver', 'is_experienced', 'past_num_of_claims',\n",
    "    'claims_per_year', 'has_past_claims', 'has_multiple_claims', 'vehicle_mileage', 'mileage_per_year',\n",
    "    'mileage_log', 'is_high_mileage', 'annual_income_capped', 'annual_income_log', 'vehicle_price_capped',\n",
    "    'vehicle_price_log', 'vehicle_weight_capped', 'vehicle_weight_log', 'claim_est_payout_capped',\n",
    "    'claim_est_payout_log', 'payout_to_income', 'payout_to_price', 'income_to_price', 'is_high_income',\n",
    "    'is_expensive_car', 'is_large_payout', 'is_weekend', 'is_weekday', 'is_morning', 'is_afternoon',\n",
    "    'is_evening', 'is_night', 'is_rush_hour', 'is_winter', 'is_summer'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507c84a",
   "metadata": {},
   "source": [
    "## CatBoost-Specific HPO: Optuna HPO Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cadaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_catboost_hyperparameters(X, y, selected_features, n_trials=100, n_splits=5):\n",
    "    print(f\"Running {n_trials} trials for CatBoost optimization...\")\n",
    "\n",
    "    cat_features = ['gender', 'vehicle_category', 'channel']\n",
    "    te_features = ['accident_type', 'accident_site', 'zip3', 'accident_combo']\n",
    "\n",
    "    cat_features = [f for f in cat_features if f in X.columns]\n",
    "    te_features = [f for f in te_features if f in X.columns]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "            'depth': trial.suggest_int('depth', 3, 8),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        f1_scores = []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_tr = X.iloc[train_idx].copy()\n",
    "            X_va = X.iloc[val_idx].copy()\n",
    "            y_tr = y.iloc[train_idx]\n",
    "            y_va = y.iloc[val_idx]\n",
    "\n",
    "            X_te_dummy = X_va.copy()  # Create dummy test for target encoding\n",
    "\n",
    "            te_names = target_encode(\n",
    "                X_tr, y_tr, X_va, X_te_dummy, te_features, smoothing=30)\n",
    "            features = list(set(selected_features + cat_features + te_names))\n",
    "            features = [f for f in features if f in X_tr.columns]\n",
    "\n",
    "            # Label Encode any remaining object types\n",
    "            for col in features:\n",
    "                if X_tr[col].dtype == 'object':\n",
    "                    le = LabelEncoder()\n",
    "                    all_vals = pd.concat([X_tr[col], X_va[col]]).unique()\n",
    "                    le.fit(all_vals)\n",
    "                    X_tr[col] = le.transform(X_tr[col])\n",
    "                    X_va[col] = le.transform(X_va[col])\n",
    "\n",
    "            smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "            X_tr_res, y_tr_res = smote.fit_resample(X_tr[features], y_tr)\n",
    "\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=1000,\n",
    "                random_state=42,\n",
    "                verbose=0,\n",
    "                **params\n",
    "            )\n",
    "            model.fit(X_tr_res, y_tr_res, eval_set=(X_va[features], y_va),\n",
    "                      early_stopping_rounds=100, verbose=False)\n",
    "\n",
    "            probs = model.predict_proba(X_va[features])[:, 1]\n",
    "\n",
    "            # Find best F1 threshold\n",
    "            best_f1 = 0\n",
    "            for thr in np.linspace(0.2, 0.4, 21):\n",
    "                preds = (probs >= thr).astype(int)\n",
    "                f1 = f1_score(y_va, preds)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "\n",
    "            f1_scores.append(best_f1)\n",
    "\n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize', study_name='catboost_optimization')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(f\"\\nBest F1: {study.best_value:.5f}\")\n",
    "    print(f\"Best CatBoost params:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b804a684",
   "metadata": {},
   "source": [
    "## F1-Weighted Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a644554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_weighted_ensemble(X, y, X_test, selected_features, lgbm_params, catboost_params, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train LGBM + XGB + CatBoost and combine with weighted average\n",
    "    Weights based on individual OOF F1 scores\n",
    "    \"\"\"\n",
    "    print(\"Starting weighted ensemble training...\")\n",
    "\n",
    "    cat_features = ['gender', 'vehicle_category', 'channel']\n",
    "    te_features = ['accident_type', 'accident_site', 'zip3', 'accident_combo']\n",
    "\n",
    "    cat_features = [f for f in cat_features if f in X.columns]\n",
    "    te_features = [f for f in te_features if f in X.columns]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    oof_lgbm = np.zeros(len(X))\n",
    "    oof_xgb = np.zeros(len(X))\n",
    "    oof_cat = np.zeros(len(X))\n",
    "\n",
    "    test_lgbm = np.zeros(len(X_test))\n",
    "    test_xgb = np.zeros(len(X_test))\n",
    "    test_cat = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"--- Fold {fold}/{n_splits} ---\")\n",
    "\n",
    "        X_tr = X.iloc[train_idx].copy()\n",
    "        X_va = X.iloc[val_idx].copy()\n",
    "        X_te = X_test.copy()\n",
    "        y_tr = y.iloc[train_idx]\n",
    "        y_va = y.iloc[val_idx]\n",
    "\n",
    "        te_names = target_encode(\n",
    "            X_tr, y_tr, X_va, X_te, te_features, smoothing=30)\n",
    "        features = list(set(selected_features + cat_features + te_names))\n",
    "        features = [f for f in features if f in X_tr.columns]\n",
    "\n",
    "        # Label Encode any remaining object types\n",
    "        for col in features:\n",
    "            if X_tr[col].dtype == 'object':\n",
    "                le = LabelEncoder()\n",
    "                all_vals = pd.concat(\n",
    "                    [X_tr[col], X_va[col], X_te[col]]).unique()\n",
    "                le.fit(all_vals)\n",
    "                X_tr[col] = le.transform(X_tr[col])\n",
    "                X_va[col] = le.transform(X_va[col])\n",
    "                X_te[col] = le.transform(X_te[col])\n",
    "\n",
    "        smote = SMOTE(sampling_strategy=0.5, random_state=42+fold)\n",
    "        X_tr_res, y_tr_res = smote.fit_resample(X_tr[features], y_tr)\n",
    "\n",
    "        # Model 1: LightGBM\n",
    "        model_lgbm = lgb.LGBMClassifier(\n",
    "            n_estimators=2000, random_state=42+fold, n_jobs=-1, verbose=-1, **lgbm_params\n",
    "        )\n",
    "        model_lgbm.fit(X_tr_res, y_tr_res, eval_set=[(X_va[features], y_va)],\n",
    "                       callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "        oof_lgbm[val_idx] = model_lgbm.predict_proba(X_va[features])[:, 1]\n",
    "        test_lgbm += model_lgbm.predict_proba(X_te[features])[:, 1] / n_splits\n",
    "\n",
    "        # Model 2: XGBoost\n",
    "        model_xgb = xgb.XGBClassifier(\n",
    "            n_estimators=2000, learning_rate=lgbm_params['learning_rate'],\n",
    "            max_depth=lgbm_params['max_depth'], subsample=lgbm_params['subsample'],\n",
    "            colsample_bytree=lgbm_params['colsample_bytree'],\n",
    "            reg_alpha=lgbm_params['reg_alpha'], reg_lambda=lgbm_params['reg_lambda'],\n",
    "            random_state=42+fold, n_jobs=-1, eval_metric='logloss'\n",
    "        )\n",
    "        model_xgb.fit(X_tr_res, y_tr_res, eval_set=[\n",
    "                      (X_va[features], y_va)], verbose=False)\n",
    "        oof_xgb[val_idx] = model_xgb.predict_proba(X_va[features])[:, 1]\n",
    "        test_xgb += model_xgb.predict_proba(X_te[features])[:, 1] / n_splits\n",
    "\n",
    "        # Model 3: CatBoost (with optimized params)\n",
    "        model_cat = CatBoostClassifier(\n",
    "            iterations=2000, random_state=42+fold, verbose=0, **catboost_params\n",
    "        )\n",
    "        model_cat.fit(X_tr_res, y_tr_res, eval_set=(X_va[features], y_va),\n",
    "                      early_stopping_rounds=150, verbose=False)\n",
    "        oof_cat[val_idx] = model_cat.predict_proba(X_va[features])[:, 1]\n",
    "        test_cat += model_cat.predict_proba(X_te[features])[:, 1] / n_splits\n",
    "\n",
    "    # Calculate individual OOF F1 scores\n",
    "    f1_lgbm = max([f1_score(y, (oof_lgbm >= t).astype(int))\n",
    "                  for t in np.linspace(0.2, 0.4, 41)])\n",
    "    f1_xgb = max([f1_score(y, (oof_xgb >= t).astype(int))\n",
    "                 for t in np.linspace(0.2, 0.4, 41)])\n",
    "    f1_cat = max([f1_score(y, (oof_cat >= t).astype(int))\n",
    "                 for t in np.linspace(0.2, 0.4, 41)])\n",
    "\n",
    "    print(f\"\\nIndividual model F1 scores:\")\n",
    "    print(f\"  LightGBM : {f1_lgbm:.5f}\")\n",
    "    print(f\"  XGBoost  : {f1_xgb:.5f}\")\n",
    "    print(f\"  CatBoost : {f1_cat:.5f}\")\n",
    "\n",
    "    # Calculate weights based on F1 scores\n",
    "    total_f1 = f1_lgbm + f1_xgb + f1_cat\n",
    "    w_lgbm = f1_lgbm / total_f1\n",
    "    w_xgb = f1_xgb / total_f1\n",
    "    w_cat = f1_cat / total_f1\n",
    "\n",
    "    print(f\"\\nWeights (based on F1):\")\n",
    "    print(f\"  LightGBM : {w_lgbm:.3f}\")\n",
    "    print(f\"  XGBoost  : {w_xgb:.3f}\")\n",
    "    print(f\"  CatBoost : {w_cat:.3f}\")\n",
    "\n",
    "    # Weighted average\n",
    "    oof_weighted = w_lgbm * oof_lgbm + w_xgb * oof_xgb + w_cat * oof_cat\n",
    "    test_weighted = w_lgbm * test_lgbm + w_xgb * test_xgb + w_cat * test_cat\n",
    "\n",
    "    # Find optimal threshold\n",
    "    best_f1 = 0\n",
    "    best_thr = 0.3\n",
    "    for thr in np.linspace(0.2, 0.4, 41):\n",
    "        preds = (oof_weighted >= thr).astype(int)\n",
    "        f1 = f1_score(y, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "\n",
    "    print(\n",
    "        f\"\\nWeighted Ensemble OOF F1: {best_f1:.5f} (threshold: {best_thr:.3f})\")\n",
    "    print(f\"Weighted Ensemble OOF AUC: {roc_auc_score(y, oof_weighted):.4f}\")\n",
    "\n",
    "    # Also try simple average for comparison\n",
    "    oof_simple = (oof_lgbm + oof_xgb + oof_cat) / 3\n",
    "    f1_simple = max([f1_score(y, (oof_simple >= t).astype(int))\n",
    "                    for t in np.linspace(0.2, 0.4, 41)])\n",
    "    print(f\"Simple Average F1: {f1_simple:.5f} (for comparison)\")\n",
    "\n",
    "    return oof_weighted, test_weighted, best_thr, (w_lgbm, w_xgb, w_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc62c6",
   "metadata": {},
   "source": [
    "## Main Execution Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8126d1",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca4519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: Train=17313, Test=686, Positive Rate=0.2296\n"
     ]
    }
   ],
   "source": [
    "# Split train and test\n",
    "df['claim_date'] = pd.to_datetime(df['claim_date'])\n",
    "\n",
    "test_df = df[\n",
    "    (df['claim_date'].dt.year == 2016) &\n",
    "    (df['claim_date'].dt.month == 9)\n",
    "].copy()\n",
    "\n",
    "train_df = df[\n",
    "    ~((df['claim_date'].dt.year == 2016) & (df['claim_date'].dt.month == 9))\n",
    "].copy()\n",
    "\n",
    "# Clean target variable\n",
    "train_df = train_df.dropna(subset=['subrogation'])\n",
    "y = train_df['subrogation'].astype(int)\n",
    "\n",
    "# Separate features and IDs\n",
    "X_raw = train_df.drop(columns=['subrogation', 'claim_number'])\n",
    "X_test_raw = test_df.drop(columns=['claim_number'])\n",
    "test_ids = test_df['claim_number']\n",
    "\n",
    "print(\n",
    "    f\"Data Loaded: Train={len(y)}, Test={len(test_ids)}, Positive Rate={y.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e87af3",
   "metadata": {},
   "source": [
    "### Run Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700767c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating enhanced features...\n",
      "Features created: 162\n",
      "Using SHAP-selected features: 94\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating enhanced features...\")\n",
    "X, artifacts = create_enhanced_features_v2(X_raw)\n",
    "X_test = create_enhanced_features_v2(X_test_raw, artifacts=artifacts)\n",
    "print(f\"Features created: {X.shape[1]}\")\n",
    "print(f\"Using SHAP-selected features: {len(SELECTED_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3a5a1",
   "metadata": {},
   "source": [
    "### Define Base LGBM Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a651848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM params from previous optimization\n",
    "lgbm_params = {\n",
    "    'learning_rate': 0.0227,\n",
    "    'num_leaves': 148,\n",
    "    'max_depth': 3,\n",
    "    'min_child_samples': 32,\n",
    "    'subsample': 0.7545,\n",
    "    'colsample_bytree': 0.5992,\n",
    "    'reg_alpha': 4.786,\n",
    "    'reg_lambda': 3.818\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8684eb",
   "metadata": {},
   "source": [
    "### Run HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65dbfbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING: Optimize CatBoost Hyperparameters\n",
      "================================================================================\n",
      "Running 20 trials for CatBoost optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.597231: 100%|██████████| 20/20 [03:43<00:00, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1: 0.59723\n",
      "Best CatBoost params:\n",
      "  learning_rate: 0.025983244440162156\n",
      "  depth: 4\n",
      "  l2_leaf_reg: 4.064450166017706\n",
      "  bagging_temperature: 0.8503802687451014\n",
      "  random_strength: 0.417598130576869\n",
      "  min_data_in_leaf: 30\n",
      "\n",
      "CatBoost optimization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING: Optimize CatBoost Hyperparameters\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "catboost_params = optimize_catboost_hyperparameters(\n",
    "    X, y, SELECTED_FEATURES, n_trials=20, n_splits=5\n",
    ")\n",
    "\n",
    "print(\"\\nCatBoost optimization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f756a",
   "metadata": {},
   "source": [
    "### Run Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be44fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING: Train F1-Weighted Ensemble\n",
      "================================================================================\n",
      "Starting weighted ensemble training...\n",
      "--- Fold 1/5 ---\n",
      "--- Fold 2/5 ---\n",
      "--- Fold 3/5 ---\n",
      "--- Fold 4/5 ---\n",
      "--- Fold 5/5 ---\n",
      "\n",
      "Individual model F1 scores:\n",
      "  LightGBM : 0.58958\n",
      "  XGBoost  : 0.58699\n",
      "  CatBoost : 0.59376\n",
      "\n",
      "Weights (based on F1):\n",
      "  LightGBM : 0.333\n",
      "  XGBoost  : 0.332\n",
      "  CatBoost : 0.335\n",
      "\n",
      "Weighted Ensemble OOF F1: 0.59124 (threshold: 0.330)\n",
      "Weighted Ensemble OOF AUC: 0.8377\n",
      "Simple Average F1: 0.59124 (for comparison)\n",
      "\n",
      "Ensemble training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING: Train F1-Weighted Ensemble\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "oof_weighted, test_weighted, threshold, weights = train_weighted_ensemble(\n",
    "    X, y, X_test, SELECTED_FEATURES, lgbm_params, catboost_params, n_splits=5\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ab906",
   "metadata": {},
   "source": [
    "### Generate Submission and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9833a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS & SUBMISSION\n",
      "================================================================================\n",
      "\n",
      "Weighted Ensemble F1: 0.59124\n",
      "  Test positive rate: 0.2566\n",
      "  Model weights: LGBM=0.333, XGB=0.332, CatBoost=0.335\n",
      "  Saved: submission.csv\n",
      "\n",
      "================================================================================\n",
      "Pipeline Finished.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS & SUBMISSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create submission file\n",
    "preds_weighted = (test_weighted >= threshold).astype(int)\n",
    "submission_proba = pd.DataFrame({\n",
    "    'claim_number': test_ids,\n",
    "    'subrogation_proba': test_weighted\n",
    "})\n",
    "submission_proba.to_csv('../data/submission.csv', index=False)\n",
    "\n",
    "# Final reporting\n",
    "oof_f1 = f1_score(y, (oof_weighted >= threshold).astype(int))\n",
    "\n",
    "print(f\"\\nWeighted Ensemble F1: {oof_f1:.5f}\")\n",
    "print(f\"  Test positive rate: {preds_weighted.mean():.4f}\")\n",
    "print(\n",
    "    f\"  Model weights: LGBM={weights[0]:.3f}, XGB={weights[1]:.3f}, CatBoost={weights[2]:.3f}\")\n",
    "print(f\"  Saved: submission.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Pipeline Finished.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
